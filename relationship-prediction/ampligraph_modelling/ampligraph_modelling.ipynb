{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ZCxEcInotw"
      },
      "source": [
        "# Implementation of Ampligraph\n",
        "Ampligraph is a library designed to generate knowledge graph embeddings and combine these with model-specific scoring functions to predict unseen and novel links. \n",
        "\n",
        "## Setup\n",
        "\n",
        "Ampligraph presently works only with Tensorflow 1.x which will no longer be supported as of August 1, 2022. [In accordance with issue #262 of the library repository on GitHub](https://github.com/Accenture/AmpliGraph/issues/262), an update of Ampligraph which works with Tensorflow 2.x is in development, and this notebook will be modified accordingly once this update is released."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4AyU4z_DHja"
      },
      "outputs": [],
      "source": [
        "# IF RUNNING LOCALLY: install tensorflow version lower than 2 in your working environment\n",
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOisVaOaBuNW"
      },
      "outputs": [],
      "source": [
        "!pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHYfucFEAK5a"
      },
      "outputs": [],
      "source": [
        "# import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph\n",
        "import requests\n",
        "\n",
        "from ampligraph.datasets import load_from_csv\n",
        "from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.latent_features import ComplEx\n",
        "from ampligraph.evaluation import evaluate_performance\n",
        "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
        "from ampligraph.utils import create_tensorboard_visualizations\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from scipy.special import expit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-vFmqNpBHhz"
      },
      "outputs": [],
      "source": [
        "# for running locally:\n",
        "# X = load_from_csv('.', '../data/best_models/knowledge-graph.csv', sep=',')\n",
        "\n",
        "X = load_from_csv('.', 'knowledge-graph.csv', sep=',')\n",
        "print(len(X))\n",
        "X[:5, ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MvEMiWdo5ts"
      },
      "source": [
        "## Defining train and test datasets\n",
        "Here the dataset is divided into data used for training the selected model and data which this trained model will test against for accuracy.\n",
        "\n",
        "Since all entities must be represented in the training and testing data sets by being apart of at least 1 sampled triple, the `train_test_split_no_unseen` function is used to ensure that no entity is left unrepresented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKh9GeFiCJla"
      },
      "outputs": [],
      "source": [
        "num_test = int(len(X) * (20 / 100))\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['train'], data['test'] = train_test_split_no_unseen(X, test_size=num_test, seed=0, allow_duplication=False) \n",
        "\n",
        "print('Train set size: ', data['train'].shape)\n",
        "print('Test set size: ', data['test'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWtxrYsSqlKw"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "As stated in the Ampligraph documentation, the default model parameters are: \n",
        "- **k** : the dimensionality of the embedding space ('size' of space which these embeddings will occupy).\n",
        "- **eta** ($\\eta$) : the number of negative, or false triples that must be generated at training runtime for each positive, or true triple\n",
        "- **batches_count** : the number of batches in which the training set is split during the training loop. \n",
        "  - *Context*: if you have a `csv` with 500 rows of data and you set the `batches_count` to 5, the data will be divided into 100 batches (500/5) with each batch containing 5 samples from the data).\n",
        "- **epochs** : the number of epochs to train the model for.\n",
        "  - *Context*: Epochs are complete passes through the dataset-- continuing from example above, one epoch would be 100 batches (aka 100 updates to the model). There should be more epochs than batches as the model needs to see the same data more than once in order to gauge improvement.\n",
        "- **optimizer** : the Adam optimizer, with a learning rate of 1e-3 set via the optimizer_params kwarg.\n",
        "- **loss** : pairwise loss, with a margin of 0.5 set via the loss_params kwarg.\n",
        "- **regularizer** : $L_p$ regularization with $p=2$, i.e. l2 regularization. $\\lambda$ = 1e-5, set via the regularizer_params kwarg.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWyFVclAHiMA"
      },
      "outputs": [],
      "source": [
        "model = ComplEx(batches_count=50,\n",
        "               seed=0,\n",
        "               epochs=1000, #pretty much settles down by 400\n",
        "               k=400,\n",
        "               eta=15,\n",
        "               optimizer='adam',\n",
        "               optimizer_params={'lr':1e-4},\n",
        "               loss='multiclass_nll',\n",
        "               regularizer='LP',\n",
        "               regularizer_params={'p':3, 'lambda':1e-5},\n",
        "               verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mjCE_tO6b5v"
      },
      "outputs": [],
      "source": [
        "model.fit(data['train'], early_stopping = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UootAcaLj5V"
      },
      "outputs": [],
      "source": [
        "#july15, 100 epochs\n",
        "model.fit(data['train'], early_stopping = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZQJdveAzXyH"
      },
      "source": [
        "## Evaluating the model\n",
        "\n",
        "The `evaluate_performance` function is given our test set, then outputs a series of ranks which evaluate the likelyhood which a given triple is true (1 indicating the highest likelyhood of truth)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThqSNcqTIIFj"
      },
      "outputs": [],
      "source": [
        "positives_filter = X\n",
        "\n",
        "ranks = evaluate_performance(data['test'], \n",
        "                             model=model, \n",
        "                             filter_triples=positives_filter,   # corruption strategy filter defined above \n",
        "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
        "                             verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNu8vmg_Kq7j"
      },
      "outputs": [],
      "source": [
        "print(ranks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDO32zzj029v"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "The `mrr_score` looks at how the positive triples are ranked in the `ranks` vector, and outputs the mean of these ranks. The percentage form of the rank for each individual positive triple is calculated by 1/n where n is the given rank. The MRR score is calculated by adding together all the ranks in their percentage form, and then dividing by number of positive triples that were evaluated. This gives us an idea of where the positive triples are most often being ranked by the model when it is evaluating for truth.\n",
        "- In [the example given in the Ampligraph documentation](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.mrr_score.html), the first triple is initially ranked 2, which becomes 1/2 = **0.5**. The second triple is initially ranked 1, which becomes 1/1 = **1**. To calculate the MRR score: (0.5 + 1) / 2 (total number of triples evaluated).\n",
        "\n",
        "The `hits_at_n_score` indicates how many times on average a true triple was ranked in the top-N ('n' being the value we indicate). This tells us how accurately our model is predicting true relationships.\n",
        "- An explanation of top-N accuracy can be found [here](https://stats.stackexchange.com/q/331508)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoXUGJ0UKuZq"
      },
      "outputs": [],
      "source": [
        "mrr = mrr_score(ranks)\n",
        "print(\"MRR: %.2f\" % (mrr))\n",
        "\n",
        "hits_10 = hits_at_n_score(ranks, n=10)\n",
        "print(\"Hits@10: %.2f\" % (hits_10))\n",
        "hits_3 = hits_at_n_score(ranks, n=3)\n",
        "print(\"Hits@3: %.2f\" % (hits_3))\n",
        "hits_1 = hits_at_n_score(ranks, n=1)\n",
        "print(\"Hits@1: %.2f\" % (hits_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9PEx9V1COSr"
      },
      "source": [
        "## Predicting New Links\n",
        "\n",
        "Link prediction allows us to infer missing links in a graph. To allow for link prediction to occur, the model is presented with a series of candidate statements and told to evaluate the likelyhood that they are true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxlBZotSK6ue"
      },
      "outputs": [],
      "source": [
        "X_unseen = np.array([\n",
        "  ['Giacomo Medici', 'employed', 'Marion True'],\n",
        "  ['Giacomo Medici','sold_antiquities_to', 'Marion True'],\n",
        "  ['Marion True', 'bought_from', 'Giacomo Medici'],\n",
        "  ['Roger Cornelius Russell Yorke', 'bought_from', 'Robin Symes'],\n",
        "  ['Fritz Bürki', 'sold_antiquities_to', 'Leon Levy'],\n",
        "  ['Gianfranco Becchina', 'partnered', 'Hischam Aboutaam'],\n",
        "  ['Robert Hecht', 'sold_antiquities_to', 'Barbara Fleischman']\n",
        "])\n",
        "\n",
        "\n",
        "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))\n",
        "\n",
        "ranks_unseen = evaluate_performance(\n",
        "    X_unseen, \n",
        "    model=model, \n",
        "    filter_unseen=True,\n",
        "    filter_triples=unseen_filter,   # corruption strategy filter defined above \n",
        "    corrupt_side = 's+o',\n",
        "    use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "scores = model.predict(X_unseen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31BaR9lCK8Jc"
      },
      "outputs": [],
      "source": [
        "probs = expit(scores)\n",
        "\n",
        "rankings = pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
        "                      ranks_unseen, \n",
        "                      np.squeeze(scores),\n",
        "                      np.squeeze(probs))), \n",
        "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeBypLp5rbyj"
      },
      "outputs": [],
      "source": [
        "#  inspect the scores \n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "pd.set_option('max_rows', 350)\n",
        "rankings = rankings.reset_index(drop=True)\n",
        "rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh47kb7zW37X"
      },
      "outputs": [],
      "source": [
        "# train/evaluation splits the data, which allows us to evaluate the accuracy of the model\n",
        "# so now, train a model on the complete knowledge graph, THEN do discovery\n",
        "\n",
        "model.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZZgYVFlRrjY"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features import save_model, restore_model\n",
        "\n",
        "# for running locally:\n",
        "# save_model(model, '../data/best_models/best_model.pkl')\n",
        "save_model(model, 'best_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGA7k4BZcdOB"
      },
      "outputs": [],
      "source": [
        "# reload a model from pickle\n",
        "from ampligraph.latent_features import restore_model\n",
        "\n",
        "# for running locally:\n",
        "# model = restore_model('../data/best_models/best_model.pkl')\n",
        "model = restore_model('./best_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRuxm4RqBque"
      },
      "outputs": [],
      "source": [
        "from ampligraph.discovery import discover_facts\n",
        "\n",
        "# top_n=3 the cutoff for rank to be considered true\n",
        "discover_facts(X, model, top_n=1, max_candidates=20000, strategy='entity_frequency', target_rel='bought_from', seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUNluClS-0Op"
      },
      "outputs": [],
      "source": [
        "# lets score that then, after cleaning out the logically unsound and the already existing statements\n",
        "# statements below are compiled from every strategy except random & exhaustive in the first rank\n",
        "\n",
        "X_unseen = np.array([\n",
        "  ['Benjamin Bishop Johnson', 'bought_from', 'Fred Drew'],\n",
        "  ['Charles Craig', 'bought_from', 'David Swetnam'],\n",
        "  ['Dietrich von Bothmer', 'bought_from', 'Gianfranco Becchina'],\n",
        "  ['Giacomo Medici', 'bought_from', 'Nikolas Koutoulakis'],\n",
        "  ['Harry Brown', 'bought_from', 'Johnnie Brown Fell'],\n",
        "  ['Hydra Gallery', 'bought_from', \"Antonio ‘Nino' Savoca\"],\n",
        "  ['J Paul Getty Museum', 'bought_from', 'Frieda Tchacos'],\n",
        "  ['J Paul Getty Museum', 'bought_from', 'Samuel Schweitzer'],\n",
        "  ['Joel Malter', 'bought_from', 'Marquis of Tavistock'],\n",
        "  ['Leon Levy', 'bought_from', 'Fritz Bürki'],\n",
        "  ['Leon Levy', 'bought_from', 'Fritz Bürki'],\n",
        "  ['Leon Levy', 'bought_from', 'Fritz Bürki'],\n",
        "  ['Leonardo Patterson', 'bought_from', 'Clive Hollinshead'],\n",
        "  ['Marion True', 'bought_from', 'Giacomo Medici'],\n",
        "  ['Pereda', 'bought_from', 'J Paul Getty Museum'],\n",
        "  ['Robert Hecht', 'bought_from', 'Robin Symes'],\n",
        "  ['Roger Cornelius Russell Yorke', 'bought_from', 'Harry Brown'],\n",
        "  ['Roger Cornelius Russell Yorke', 'bought_from', 'Harry Brown'],\n",
        "  ['Vaman Ghiya', 'bought_from', 'David Bernstein']\n",
        "])\n",
        "\n",
        "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))\n",
        "\n",
        "ranks_unseen = evaluate_performance(\n",
        "    X_unseen, \n",
        "    model=model, \n",
        "    filter_unseen=True,\n",
        "    filter_triples=unseen_filter,   # corruption strategy filter defined above \n",
        "    corrupt_side = 's+o',\n",
        "    use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "scores = model.predict(X_unseen)\n",
        "\n",
        "probs = expit(scores)\n",
        "\n",
        "rankings = pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
        "                      ranks_unseen, \n",
        "                      np.squeeze(scores),\n",
        "                      np.squeeze(probs))), \n",
        "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\")\n",
        "\n",
        "# inspect the scores \n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "pd.set_option('max_rows', 350)\n",
        "rankings = rankings.reset_index(drop=True)\n",
        "rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw6pyTlIyV2N"
      },
      "outputs": [],
      "source": [
        "from ampligraph.discovery import find_nearest_neighbours\n",
        "neighbors, dist = find_nearest_neighbours(model,\n",
        "                                           entities=['Giacomo Medici','Marion True','Robin Symes'],\n",
        "                                           n_neighbors=5)\n",
        "\n",
        "print(neighbors, dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgpxLjhEAEj9"
      },
      "outputs": [],
      "source": [
        "from ampligraph.discovery import discover_facts\n",
        "# top_n=3 the cutoff for rank to be considered true\n",
        "\n",
        "# sold_antiquities_to is inverse of bought_from\n",
        "# try 'partnered'\n",
        "\n",
        "p_result = discover_facts(X, model, top_n=1, max_candidates=20000, strategy='cluster_squares', target_rel='partnered', seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrU0KjP7qLEB"
      },
      "outputs": [],
      "source": [
        "p_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zRyQldAcUUY"
      },
      "outputs": [],
      "source": [
        "# lets score that then, after cleaning out the logically unsound and the already existing statements\n",
        "\n",
        "X_unseen = np.array([\n",
        "  ['Clive Hollinshead', 'partnered', 'Harry Brown'],\n",
        "  ['Charles Craig', 'partnered', 'Roger Cornelius Russell Yorke'],\n",
        "  ['United States Customs', 'partnered', 'Royal Canadian Mounted Police'],\n",
        "  ['Mario Bruno', 'partnered', 'Giacomo Medici'],\n",
        "  ['Roger Cornelius Russell Yorke', 'partnered', 'Charles Craig'],\n",
        "  ['Anton Tkalec', 'partnered', 'Mansur Mokhtarzade'],\n",
        "  ['Michael Kelly', 'partnered', 'Miguel de Osma Berckemeyer'],\n",
        "  ['Robert Hecht', 'partnered', 'Robin Symes'],\n",
        "  ['Clive Hollinshead', 'partnered', 'Harry Brown']\n",
        "])\n",
        "\n",
        "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))\n",
        "\n",
        "ranks_unseen = evaluate_performance(\n",
        "    X_unseen, \n",
        "    model=model, \n",
        "    filter_unseen=True,\n",
        "    filter_triples=unseen_filter,   # corruption strategy filter defined above \n",
        "    corrupt_side = 's+o',\n",
        "    use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "scores = model.predict(X_unseen)\n",
        "\n",
        "probs = expit(scores)\n",
        "\n",
        "rankings = pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
        "                      ranks_unseen, \n",
        "                      np.squeeze(scores),\n",
        "                      np.squeeze(probs))), \n",
        "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\")\n",
        "\n",
        "# inspect the scores \n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "pd.set_option('max_rows', 350)\n",
        "rankings = rankings.reset_index(drop=True)\n",
        "rankings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExOXDI8RfSyZ"
      },
      "outputs": [],
      "source": [
        "from ampligraph.discovery import query_topn\n",
        "\n",
        "query_topn(model, top_n=3,\n",
        "           head='Marion True', relation='partnered', tail=None,\n",
        "           ents_to_consider=None, rels_to_consider=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMynAYtYEY5g"
      },
      "source": [
        "## Tensorboard Visualizing \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csQDlzX93EXI"
      },
      "outputs": [],
      "source": [
        "# reload a model from pickle\n",
        "from ampligraph.latent_features import restore_model\n",
        "\n",
        "# for running locally:\n",
        "# model = restore_model('../data/best_models/best_model.pkl')\n",
        "model = restore_model('./best_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2mignN9LnG4"
      },
      "outputs": [],
      "source": [
        "from ampligraph.utils import create_tensorboard_visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD2NDafRMpaR"
      },
      "outputs": [],
      "source": [
        "create_tensorboard_visualizations(model, '4thtc_embeddings')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmvUmQZM1DU"
      },
      "outputs": [],
      "source": [
        "# restart the runtime to reset tensorflow to 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs7Yt5snLru9"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAoeEuK7NQi-"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ./kg_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y4MQkE6sg7v"
      },
      "source": [
        "Another codebase for further visualizations\n",
        "\n",
        "https://github.com/roosyay/CoDa_Hypotheses/blob/master/4.%20Visualisation.ipynb\n",
        "\n",
        "https://link.springer.com/chapter/10.1007/978-3-030-77385-4_28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Td-VfidIcq"
      },
      "outputs": [],
      "source": [
        "!zip -r out.zip tc_embeddings/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of correct ampligraph for article.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
